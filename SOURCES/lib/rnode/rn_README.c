
/*
 * rnodes ("resource nodes") are the entities which display properties
 * of a resource manager.
 *
 * rnodes are an in-memory representation of the union of resource manager
 * information and other state collected from the running system, including
 * a small number of writable and modifiable nodes containing dynamic state
 * These nodes are relatively resource manager independent, although
 * this implementation is tied closely to the slurm resource manager.
 */


/*
 * The file system structure of an rnode tree is pre-defined.
 *
 * The fs does not own its own state; it obtains it from the resource manager
 * or in some cases from pre-existing BackingStore.
 * There are almost no ops which modify state, but...
 *
 * Exceptions to this are:
 * - directories may have extended attributes set on them
 *
 * - Nodes (ex. "cluster", possibly "partition") may be controlled,
 *   in which case they will have a control function (a leaf node file)
 *   which accepts commands. Commands are generated by opening the
 *   leaf node and writing a command to it.
 * 
 *   For now this is just the root or cluster node, which could:
 *    "umount" - as requested
 *    future:
 *      "exit" - a synonym for unmount, usually meant to indicate another
 *               instance is about to (re)mount a slurmfs instance
 *
 *      "reread"/"reload" - state from slurm (in a future version) 
 *
 *   If this were not implemented as a fuse module, and if it were
 *   implemented in the VFS layer, then "remount" would substitute
 *   for "exit."
 *
 *   ClusterController view:
 *   - This view is visible when the mount command is invoked on a node whose
 *     hostname matches the ControllerName in the $SLURM_CONF file
 *   - It shows states for all nodes known to the controller.
 *   - Extended attributes ("xattr") are arbitrary values, but may be used
 *     to store the textual representation of an SELinux context.
 *   - The "BackingStore" option makes this extended attribute into a kernel object,
 *     which provides an enforcement point. This can be used by an arbitrary
 *     enforcement policy, such as grsecurity, SELinux, etc.
 *     If this prototype were to be put into production, then this code could
 *     move to an in-kernel VFS, directly implementing the enforcement point(s).
 *   - If the in-kernel VFS-layer of fuse does not support extended attributes
 *     then the setxattr/getxattr syscalls are made visible through a similar
 *     control leaf on the slurmfs entries.
 *
 * default file system tree shape:
 *   <name> indicates a value to be filled in from the resource manager
 *   "dir" indicates a literal label such as the "jobs" directory name
 *         or an attribute name, such as "update_time"
 *
 *
 * root ----- <cluster-name> + "jobs" -+- <jobid1> --- "jobsteps" -+- <job1:step1> 
 *    |                  |   |         |     |                     |
 * "attributes"          |   |         |    ...                   -+- <job1:step2>
 *    |                  |   |         |
 *    |                  |   |         +- <jobid2> --- "jobsteps" -+- <job2:step1>
 *    +- "BackingStore"  |   |               |                     |
 *    |                  |   |            "attributes"*           -+- <job2:step2>
 *    +- "control"       |   |               +- "context"*         |
 *    |   +- "umount"    |   |               +- "name"            -+- <job2:step3> 
 *    |   +- "effluviate"|   |               +- <job2-attr2>       |
 *    |   +- "check"     |   |               +- "signature"       ...
 *    |                  |   |              ...
 *    |                  |   |
 *    +- "Debug"         |   + "partitions" + <partition1>  - "nodes" -+- <node1> -+- n1-attr1
 *    |                  |                  |                          |           |
 *    +- "ClusterName"   |                  + <partition2> - "nodes"   |           +- n1-attr2
 *    |                  |                                      |      |           |           
 *    +- "context"  "attributes"                               ...     |           +- "state"
 *    |                  +  "SlurmCtldTimeout"                         |                 |
 *    |                  |                                            -+- <node2>      alloc
 *    +- "defcontext"    +- "SlurmTimeout"                             |                 |
 *    |                                                               ...           "jobidX",
 *    +- "Hostname"                                                                 if alloc
 *    +- ...
 *    +- "pid"
 *    +- "pidfile"
 *    +- "unmountwait"
 *
 * This tree structure may vary depending upon the version and security model.
 * There is room for generalized feature sets within a partition. (XXXFUTURE)
 *
 * Most files are owned by the SlurmUser, except for jobs which are owned by
 * the submitting userid.
 *
 * Although this is a view of the data presented by the authoritative data owner,
 * the resource manager, the node marked '*' is writable and can store state.
 * The 'BackingStore' is used to preserve non-resource-manager state across (re)mounts. 
 *
 *   ClusterNode view:
 *   - The node view may be pruned version of the controller's view. It is visible
 *     when the mount command is invoked on a node whose hostname matches one of
 *     the slurm HostList names. It shows either the stem to the nodename, or to
 *     the currently allocated job, only.
 *
 *   - Alternatively, this need not be a feature of slurmfs, just the (NFS-)exported
 *     pruned version for each node, enforced by NFS MAC and DAC
 *
 *   This is keyed from the cluster node's hostname and ControllerName parameters.
 */

/*
 * attributes are obtained from the resource manager state structures
 * for slurm: partition_info_t, job_info_t, node_info_t, jobstep_info_t,
 * in most cases the attributes are derivative of resource manager-specific state
 * for example, the RNL_ALLOCJOB is a constructed attribute based upon
 * the node and job state. In this case, the relevant rn_buildfn() is used
 * to construct the derived datum.
 *
 * XXXFUTURE use a preprocessor that digests the resource manager API
 * XXXFUTURE data descriptor and structures to generate these tables
 * XXXFUTURE it would be implemented as a dependency of the resource manager API
 * XXXFUTURE that defines the resource manager source structure and state
 *
 * due to the heavy dependence on the individual resource manager, this is
 * actually defined in slu_rmfs.c
 */

  
/*
 * 
 * The following build functions are called via rnode_buildtab with a key
 * of the type of node to be built.
 *
 * All are called with a signature of:
 *  rnode_t * rn_buildfn(rnode_t *p_parent, unsigned long opaque)
 *
 * The p_parent is the rnode in which the new node is to be created.
 * In the case of the file system root, this may be NULL.
 * In all other cases, this is required by the buildfn().
 *
 * The second argument is up to the build function to interpret.
 * The provisioning build function, the common rnode constructor(),
 * interprets this as the number of new nodes to allocate space.
 *
 * Most buildfn's allocate the space for their children so that the parent
 * nodes can define which nodes are which of their children. The pointer
 * to the pre-allocated individual embryonic rnode is passed in to
 * the type-specific build function.
 *
 * On successful construction of an rnode, a pointer to that rnode is returned.
 * Errors are indicated either by a (rnode_t *) NULL return.
 * If multiple rnodes were constructed, this is the pointer to the first
 * of a table of rnodes. The last entry of each of these sequences is
 * usually an empty rnode, as a memory guard and debugging aid, although
 * this is not enforced.
 *
 * The final build function is actually the rnode destructor/de-allocator.
 *
 */

/*
 * rnode constructors ("rn_mk*") follow the general pattern of:
 *   1. determine the number of rnodes to allocate
 *      usually this is the number of children rnodes
 *      children rnodes are subdirectories and attributes
 *      the directories are usually placed first 
 *      only the rootfs (RND_ROOT) node allocates space for itself
 *
 *   2. call the generic provisioner (rn_provision()) to
 *      perform the memory allocation, manage the rnode serial #
 *
 *   3. fill in characteristics specific to the node type itself
 *      via a call to rn_cast()
 *
 *   4. a. the rn_cast() function fills out the information known
 *         to the parent about the child
 *      b. it then calls the per-type build function of the child
 * >>>     which recalls rn_cast() on itself, as in (1) <<< 
 *
 * All builder functions are called with their parent as the 1st argument
 * and a second argument that is up to the called builder to interpret.
 *
 * The 2nd arg is usually the pre-provisioned embryonic rnode, as allocated
 * by the parent rnode, except for the rn_provision() and rn_raze() build
 * functions which interpret this 2nd arg differently.
 *
 * The names of the explicit directories ("jobs", "partitions", ...) are collected
 * from the slurmfs_config[] configuration parameter table. These names match the
 * rnode_buildtab[] entry.
 *
 */




